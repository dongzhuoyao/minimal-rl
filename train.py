"""
FlowGRPO (Flow-based Group Relative Policy Optimization) Training Script.

================================================================================
INTRODUCTION TO FLOWGRPO
================================================================================

FlowGRPO is a reinforcement learning algorithm for training flow matching models
(like diffusion models) using reward-based optimization. It combines:

1. **Flow Matching**: A generative modeling technique that learns to transform
   noise into data by following a continuous probability flow (ODE).

2. **Group Relative Policy Optimization (GRPO)**: An RL algorithm similar to PPO
   but with group-based advantage computation that reduces variance and improves
   training stability.

================================================================================
HOW FLOWGRPO WORKS (According to this implementation)
================================================================================

The training process follows these main phases:

1. **SAMPLING PHASE** (Lines 346-445):
   - For each batch of labels, sample multiple trajectories (num_samples_per_label)
   - Each trajectory is generated by:
     * Starting from noise: x_0 ~ N(0, I)
     * Following the flow: x_{t+1} = x_t + dt * v_θ(x_t, t, label)
     * Where v_θ is the velocity field predicted by the model
   - Compute log probabilities for each step: log p ≈ -0.5 * ||v||² * dt
   - Store full trajectories for later training

2. **REWARD COMPUTATION** (Lines 376-409):
   - Convert generated images to [0, 1] range
   - Compute rewards using reward function: r = R(image, prompt)
   - Handle NaN/Inf values for numerical stability

3. **ADVANTAGE COMPUTATION** (Lines 411-414):
   - Group samples by label (digit class 0-9)
   - For each group, compute advantages:
     A = (r - μ_group) / σ_group
   - This makes advantages relative to the group mean, reducing variance
   - Advantages are tracked across training using PerLabelStatTracker

4. **TRAINING PHASE** (Lines 447-599):
   - For each timestep in the trajectory:
     * Compute new log probability: log p_new = log p_θ(x_t → x_{t+1})
     * Compute importance ratio: ratio = exp(log p_new - log p_old)
     * Apply GRPO loss (clipped PPO-style):
       L = -mean(max(
           A * ratio,                    # Unclipped
           A * clip(ratio, 1-ε, 1+ε)     # Clipped
       ))
     * Optional KL penalty: L += β * KL(π_new || π_old)
   - Average loss across all timesteps
   - Backpropagate and update model parameters

================================================================================
KEY COMPONENTS
================================================================================

1. **PerLabelStatTracker** (Lines 38-131):
   - Tracks reward statistics per label (digit class)
   - Computes group-relative advantages: A = (r - μ) / σ
   - Supports different advantage types: 'grpo', 'rwr', 'sft', 'dpo'

2. **sample_with_logprob** (Lines 134-202):
   - Samples trajectories using Euler integration
   - Computes log probabilities for each step
   - Supports gradient-enabled sampling for training

3. **compute_log_prob_at_timestep** (Lines 205-240):
   - Computes log probability of transition x_t → x_{t+1}
   - Uses velocity prediction: log p ≈ -0.5 * ||v_pred||² * dt

4. **Main Training Loop** (Lines 243-720):
   - Alternates between sampling and training phases
   - Uses importance sampling with clipping (PPO-style)
   - Includes gradient clipping, checkpointing, and wandb logging

================================================================================
KEY DIFFERENCES FROM STANDARD PPO
================================================================================

1. **Group-based Advantages**: Advantages are computed relative to the mean reward
   within each label group, not globally. This reduces variance when different
   labels have different reward scales.

2. **Trajectory-based Training**: The policy is updated at each timestep of the
   flow matching trajectory, not just at the end. This allows fine-grained
   control over the generation process.

3. **Flow Matching Dynamics**: The policy predicts velocity fields v_θ(x, t, label)
   that define the ODE flow, rather than directly predicting actions.

================================================================================
USAGE
================================================================================

    python train.py                          # Use default config
    python train.py training.max_steps=5000  # Override specific parameter
    python train.py training.batch_size=8 training.num_samples_per_label=4

================================================================================
"""
import hydra
from hydra.core.hydra_config import HydraConfig
from omegaconf import DictConfig, OmegaConf
from pathlib import Path
import torch
from torch.utils.data import DataLoader
from torchvision.utils import make_grid
import sys
from tqdm import tqdm
import os
import wandb
import numpy as np
from PIL import Image
from collections import defaultdict

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from dataset import MNISTDataset
from model import SimpleUNet
from rewards.mnist_rewards import get_recommended_reward_config
from utils import ensure_image_shape, to_scalar

# Constants
MNIST_IMG_SIZE = 28
MNIST_SIGNAL_DIM = MNIST_IMG_SIZE * MNIST_IMG_SIZE  # 784


def create_image_grid(images: list, labels: list, rewards: list, advantages: list = None) -> tuple:
    """
    Create a grid of images with captions for wandb logging.

    Args:
        images: List of image tensors [1, 28, 28] or [28, 28]
        labels: List of label values
        rewards: List of reward values
        advantages: Optional list of advantage values

    Returns:
        Tuple of (PIL image, combined caption string) or (None, None) if failed
    """
    if not images:
        return None, None

    processed_images = []
    captions = []

    for i, img in enumerate(images):
        try:
            img_cpu = img.cpu() if img.is_cuda else img
            img_shaped = ensure_image_shape(img_cpu, 1, MNIST_IMG_SIZE, MNIST_IMG_SIZE)
            processed_images.append(img_shaped)

            label = labels[i].item() if hasattr(labels[i], 'item') else labels[i]
            reward = to_scalar(rewards[i])

            if advantages is not None:
                adv = to_scalar(advantages[i])
                captions.append(f"L:{label} R:{reward:.4f} A:{adv:.4f}")
            else:
                captions.append(f"L:{label} R:{reward:.4f}")
        except (ValueError, RuntimeError) as e:
            continue

    if not processed_images:
        return None, None

    # Stack and create grid
    images_tensor = torch.stack(processed_images)  # [N, 1, 28, 28]
    images_tensor = torch.clamp(images_tensor, 0.0, 1.0)
    grid = make_grid(images_tensor, nrow=4, normalize=False, pad_value=1.0)

    # Convert to PIL
    grid_np = grid.permute(1, 2, 0).cpu().numpy()
    grid_np = np.clip(grid_np, 0.0, 1.0)
    grid_np = (grid_np * 255).astype(np.uint8)
    grid_pil = Image.fromarray(grid_np, mode='RGB')

    return grid_pil, " | ".join(captions)


class PerLabelStatTracker:
    """
    Tracks statistics per label for computing advantages in GRPO.
    
    This is a core component of FlowGRPO that implements group-relative advantage
    computation. Instead of computing advantages globally, it groups samples by
    label (digit class) and computes advantages relative to each group's mean.
    
    Key idea: A = (r - μ_group) / σ_group
    
    This reduces variance when different labels have different reward scales,
    making training more stable.
    
    Based on original_impl/flow_grpo/stat_tracking.py
    
    Attributes:
        global_std (bool): If True, use global std across all labels instead of
                          per-label std. Default: False (use per-label std).
        stats (dict): Dictionary mapping label -> list of historical rewards
        history_labels (set): Set of all labels seen during training
    """
    def __init__(self, global_std=False):
        self.global_std = global_std
        self.stats = {}
        self.history_labels = set()

    def update(self, labels, rewards, type='grpo'):
        """
        Compute advantages from rewards using per-label statistics.
        
        Args:
            labels: Array of labels [batch_size]
            rewards: Array of rewards [batch_size]
            type: Type of advantage computation ('grpo', 'rwr', 'sft', 'dpo')
        
        Returns:
            advantages: Array of advantages [batch_size]
        """
        labels = np.array(labels)
        rewards = np.array(rewards, dtype=np.float64)
        unique = np.unique(labels)
        advantages = np.zeros_like(rewards)
        
        for label in unique:
            label_rewards = rewards[labels == label]
            if label not in self.stats:
                self.stats[label] = []
            # Convert to list if it's a numpy array (from previous update)
            if isinstance(self.stats[label], np.ndarray):
                self.stats[label] = self.stats[label].tolist()
            self.stats[label].extend(label_rewards)
            self.history_labels.add(int(label))
        
        for label in unique:
            self.stats[label] = np.stack(self.stats[label])
            label_rewards = rewards[labels == label]
            mean = np.mean(self.stats[label], axis=0, keepdims=True)
            if self.global_std:
                std = np.std(rewards, axis=0, keepdims=True) + 1e-4
            else:
                std = np.std(self.stats[label], axis=0, keepdims=True) + 1e-4
            
            if type == 'grpo':
                advantages[labels == label] = (label_rewards - mean) / std
            elif type == 'rwr':
                advantages[labels == label] = label_rewards
            elif type == 'sft':
                advantages[labels == label] = (torch.tensor(label_rewards) == torch.max(torch.tensor(label_rewards))).float().numpy()
            elif type == 'dpo':
                label_advantages = torch.tensor(label_rewards)
                max_idx = torch.argmax(label_advantages)
                min_idx = torch.argmin(label_advantages)
                if max_idx == min_idx:
                    min_idx = 0
                    max_idx = 1
                result = torch.zeros_like(label_advantages).float()
                result[max_idx] = 1.0
                result[min_idx] = -1.0
                advantages[labels == label] = result.numpy()
        
        return advantages

    def get_stats(self):
        avg_group_size = sum(len(v) for v in self.stats.values()) / len(self.stats) if self.stats else 0
        history_labels = len(self.history_labels)
        return avg_group_size, history_labels
    
    def clear(self):
        self.stats = {}


def sample_with_logprob(model, labels, num_steps, device, return_trajectory=False, enable_grad=False, sigma_max=80.0):
    """
    Sample trajectories with log probabilities for GRPO training.
    
    This function implements the flow matching sampling process using vector fields:
    1. Initialize: x_0 ~ N(0, I) (start from noise)
    2. For each step t in [0, 1]:
       - Convert time to sigma: sigma = t * sigma_max
       - Predict velocity field: v = model(x_t, sigma, labels)
       - Update state: x_{t+1} = x_t + dt * v (Euler integration)
       - Compute log prob: log p ≈ -0.5 * ||v||² * dt
    3. Return final images and log probabilities
    
    Flow matching uses a vector field (velocity field) to transform noise to data.
    The model predicts the velocity v_θ(x_t, sigma, label) that guides the flow.
    
    The log probabilities are used in the GRPO loss to compute importance
    ratios: ratio = exp(log p_new - log p_old)
    
    Args:
        model: Flow matching model (predicts velocity field v_θ(x, sigma, label))
        labels: [B] class labels (0-9 for MNIST)
        num_steps: Number of Euler integration steps
        device: Device to run on ('cpu' or 'cuda')
        return_trajectory: If True, return full trajectory (all intermediate states)
        enable_grad: If True, enable gradients (for training phase sampling)
        sigma_max: Maximum noise level (sigma = t * sigma_max, where t ∈ [0, 1])
    
    Returns:
        final_images: [B, 1, 28, 28] final generated images in [0, 1] range
        trajectory: List of [B, signal_dim] tensors (if return_trajectory=True)
        log_probs: [B, num_steps] log probabilities for each step
        timesteps: [B, num_steps] time values for each step
    """
    if not enable_grad:
        model.eval()
    B = labels.shape[0]

    # Initialize with noise: x_1 ~ N(0, I) (matching train0.py)
    x = torch.randn(B, 1, MNIST_IMG_SIZE, MNIST_IMG_SIZE, device=device)
    
    trajectory = [x.clone().view(B, MNIST_SIGNAL_DIM)] if return_trajectory else None
    log_probs = []
    timesteps = []
    
    # Euler integration
    dt = 1.0 / num_steps
    
    # Time steps from 1 to 0 (backwards, matching train0.py)
    timesteps_list = torch.linspace(1.0, 0.0, num_steps + 1, device=device)
    
    context = torch.enable_grad() if enable_grad else torch.no_grad()
    with context:
        for step in range(num_steps):
            # Current time t (from 1.0 to 0.0)
            t = timesteps_list[step]
            
            # Convert time t to sigma: sigma = t * sigma_max
            # t goes from 1.0 to 0.0, sigma goes from sigma_max to 0
            sigma = t * sigma_max  # scalar or broadcast to [B]
            if isinstance(sigma, torch.Tensor) and sigma.dim() == 0:
                sigma = sigma.item()
            
            # Predict velocity (matching train0.py)
            v_pred = model(x, sigma, labels)  # [B, 1, 28, 28]
            
            # Compute log probability for this step
            # For flow matching: log p ≈ -0.5 * ||v||^2 * dt
            v_flat = v_pred.view(B, MNIST_SIGNAL_DIM)
            log_prob = -0.5 * torch.sum(v_flat ** 2, dim=-1) * dt
            log_probs.append(log_prob)
            timesteps.append(torch.full((B,), t, device=device))

            # Euler step: x_{t-dt} = x_t - dt * v_pred (matching train0.py)
            x = x - dt * v_pred

            # Store trajectory if needed (flattened for consistency)
            if return_trajectory:
                trajectory.append(x.clone().view(B, MNIST_SIGNAL_DIM))
    
    log_probs = torch.stack(log_probs, dim=1)  # [B, num_steps]
    timesteps = torch.stack(timesteps, dim=1)  # [B, num_steps] - each row is the same t value
    
    # Final images are already in [B, 1, 28, 28] format
    final_images = x
    
    # Clamp to valid range [0, 1] for MNIST (matching train0.py)
    final_images = torch.clamp(final_images, 0.0, 1.0)
    
    if return_trajectory:
        return final_images, trajectory, log_probs, timesteps
    else:
        return final_images, log_probs, timesteps


def compute_log_prob_at_timestep(model, x_t, t, labels, x_next, dt, sigma_max=80.0):
    """
    Compute log probability of transitioning from x_t to x_next at timestep t.
    
    This is used during training to compute the new policy's log probability
    for importance sampling. The log probability is approximated as:
    
        log p(x_{t-dt} | x_t) ≈ -0.5 * ||v_pred||² * dt
    
    where v_pred = model(x_t_img, sigma, labels) is the predicted velocity.
    Note: We go backwards in time (t: 1.0 → 0.0), so x_next = x_t - dt * v_pred.
    
    This function is called for each timestep in the trajectory during training
    to compute log p_new, which is then compared to log p_old (from sampling)
    to compute the importance ratio.
    
    Args:
        model: SimpleUNet model (predicts velocity field)
        x_t: [B, signal_dim] current state at timestep t (flattened)
        t: [B] or [B, 1] current time value (from 1.0 to 0.0, matching train0.py)
        labels: [B] class labels (0-9 for MNIST)
        x_next: [B, signal_dim] next state (from stored trajectory, x_{t-dt})
        dt: Time step size (1.0 / num_steps)
        sigma_max: Maximum noise level (default 80.0, matching train0.py)
    
    Returns:
        log_prob: [B] log probabilities for each sample in batch
    """
    B = x_t.shape[0]
    signal_dim = x_t.shape[1]
    img_size = int(signal_dim ** 0.5)  # Assume square images
    
    # Reshape x_t to image format [B, 1, 28, 28] (matching train0.py format)
    x_t_img = x_t.view(B, 1, img_size, img_size)
    
    # Convert time t to sigma: sigma = t * sigma_max
    # t goes from 1.0 to 0.0, sigma goes from sigma_max to 0
    t_flat = t.squeeze(-1) if t.dim() > 1 else t  # [B]
    if isinstance(t_flat, torch.Tensor):
        sigma = t_flat * sigma_max  # [B]
    else:
        sigma = t_flat * sigma_max  # scalar
    
    # Predict velocity (vector field) using SimpleUNet (matching train0.py)
    v_pred_img = model(x_t_img, sigma, labels)  # [B, 1, 28, 28]
    
    # Reshape velocity back to flattened format
    v_pred = v_pred_img.view(B, signal_dim)  # [B, signal_dim]
    
    # Compute log probability: -0.5 * ||v_pred||^2 * dt
    # Clip v_pred^2 to prevent overflow
    v_squared = torch.clamp(v_pred ** 2, max=1e6)  # Prevent overflow
    log_prob = -0.5 * torch.sum(v_squared, dim=-1) * dt
    
    # Clip log_prob to reasonable range to prevent extreme values
    log_prob = torch.clamp(log_prob, min=-100.0, max=100.0)
    
    return log_prob


def load_pretrained_checkpoint(model, checkpoint_path, device, strict=False):
    """
    Load a pretrained checkpoint into a model with comprehensive verification.
    
    This function handles checkpoint loading with proper error handling and
    verification. It checks for:
    - File existence
    - Checkpoint structure (must contain 'model_state_dict')
    - Key mismatches (missing/unexpected keys)
    - Parameter count verification
    
    Args:
        model: PyTorch model to load checkpoint into
        checkpoint_path: Path to the checkpoint file (.pt or .pth)
        device: Device to load checkpoint on ('cpu' or 'cuda')
        strict: If True, require exact key match. If False, allow partial loading.
                Default: False (allows partial loading with warnings)
    
    Returns:
        checkpoint: Dictionary containing the loaded checkpoint (includes 'model_state_dict',
                   'optimizer_state_dict', 'step', etc.)
    
    Raises:
        FileNotFoundError: If checkpoint file doesn't exist
        KeyError: If checkpoint is missing 'model_state_dict' key
    """
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Pretrained checkpoint not found: {checkpoint_path}")
    
    print(f"Loading pretrained checkpoint from {checkpoint_path}...")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # Verify checkpoint structure
    if 'model_state_dict' not in checkpoint:
        raise KeyError(f"Checkpoint missing 'model_state_dict' key. Available keys: {list(checkpoint.keys())}")
    
    # Get model and checkpoint state dicts
    model_state_dict = model.state_dict()
    checkpoint_state_dict = checkpoint['model_state_dict']
    
    # Check for missing and unexpected keys
    model_keys = set(model_state_dict.keys())
    checkpoint_keys = set(checkpoint_state_dict.keys())
    
    missing_keys = model_keys - checkpoint_keys
    unexpected_keys = checkpoint_keys - model_keys
    
    if missing_keys:
        print(f"WARNING: Missing keys in checkpoint: {missing_keys}")
    if unexpected_keys:
        print(f"WARNING: Unexpected keys in checkpoint (will be ignored): {unexpected_keys}")
    
    # Store a sample weight before loading to verify it changes
    sample_param_name = next(iter(model_state_dict.keys()))
    weight_before = model_state_dict[sample_param_name].clone()
    
    # Load state dict with strict parameter
    load_result = model.load_state_dict(checkpoint_state_dict, strict=strict)
    
    # Verify that weights actually changed
    weight_after = model.state_dict()[sample_param_name]
    weights_changed = not torch.equal(weight_before, weight_after)
    
    if load_result.missing_keys:
        print(f"WARNING: Some model parameters were not loaded: {load_result.missing_keys}")
    if load_result.unexpected_keys:
        print(f"WARNING: Some checkpoint parameters were not used: {load_result.unexpected_keys}")
    
    # Verify loading by checking a few parameter values
    if not missing_keys and not unexpected_keys:
        print("✓ Checkpoint loaded successfully! All keys matched.")
    elif not missing_keys:
        print("✓ Checkpoint loaded successfully! (Some unexpected keys were ignored)")
    else:
        print(f"⚠ Checkpoint loaded with warnings: {len(missing_keys)} missing keys, {len(unexpected_keys)} unexpected keys")
    
    # Verify weights were actually updated
    if weights_changed:
        print(f"✓ Model weights updated successfully (verified: {sample_param_name} changed)")
    else:
        print(f"⚠ WARNING: Model weights may not have changed! Check if checkpoint matches model architecture.")
        print(f"  Sample param '{sample_param_name}' shape: {weight_before.shape}")
        print(f"  Before: mean={weight_before.mean().item():.6f}, std={weight_before.std().item():.6f}")
        print(f"  After:  mean={weight_after.mean().item():.6f}, std={weight_after.std().item():.6f}")
    
    # Print checkpoint info
    if 'step' in checkpoint:
        print(f"  Checkpoint step: {checkpoint['step']}")
    print(f"  Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    print(f"  Checkpoint parameters: {sum(p.numel() for p in checkpoint_state_dict.values()):,}")
    
    return checkpoint


@hydra.main(version_base=None, config_path="configs", config_name="train")
def main(cfg: DictConfig):
    """
    Main FlowGRPO training function with Hydra configuration.
    
    This function implements the complete FlowGRPO training loop:
    
    1. **Setup**: Load model, dataset, reward function, optimizer
    2. **Training Loop**:
       a. **Sampling Phase**: Generate trajectories and compute rewards
       b. **Advantage Computation**: Compute group-relative advantages
       c. **Training Phase**: Update policy using GRPO loss
    3. **Logging**: Track metrics with wandb, save checkpoints
    
    The training alternates between sampling (model.eval()) and training
    (model.train()) phases. During sampling, multiple trajectories are
    generated per label. During training, the policy is updated using
    importance sampling with clipping (PPO-style).
    
    Args:
        cfg: Hydra configuration dictionary containing:
            - model: Model architecture parameters
            - training: Training hyperparameters (lr, batch_size, etc.)
            - reward: Reward function configuration
            - wandb: Weights & Biases logging configuration
            - paths: Output directory paths
    """
    
    # Print configuration
    print("=" * 60)
    print("FloWGRPO Training Configuration:")
    print("=" * 60)
    print(OmegaConf.to_yaml(cfg))
    print("=" * 60)
    
    # Initialize wandb
    if cfg.wandb.enabled:
        hydra_cfg = HydraConfig.get()
        hydra_output_dir = Path(hydra_cfg.run.dir)
        wandb_dir = hydra_output_dir / "wandb"
        os.environ["WANDB_DIR"] = str(wandb_dir)
        wandb_dir.mkdir(parents=True, exist_ok=True)
        
        wandb.init(
            project=cfg.wandb.project,
            name=cfg.wandb.run_name if cfg.wandb.run_name else cfg.run_name,
            config=OmegaConf.to_container(cfg, resolve=True),
            tags=cfg.wandb.get("tags", []),
            dir=str(wandb_dir),
        )
        print(f"Wandb initialized: {wandb.run.url}")
    
    # Set random seed
    if hasattr(cfg, 'seed'):
        torch.manual_seed(cfg.seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(cfg.seed)
        np.random.seed(cfg.seed)
    
    # Determine device
    device = cfg.training.device
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")
    device = torch.device(device)
    
    # Create output directory
    output_dir = Path(cfg.paths.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Load MNIST dataset
    print("Loading MNIST datasets...")
    dataset_dir = Path(cfg.dataset.dataset_dir)
    train_dataset = MNISTDataset(dataset_dir, split="train", download=True)
    print(f"Train samples: {len(train_dataset)}")
    
    # Create model
    print("Creating flow matching model...")
    model = SimpleUNet(
        img_channels=1,  # MNIST is grayscale
        label_dim=cfg.model.vocab_size,  # 10 classes (digits 0-9)
        time_emb_dim=cfg.model.get("time_emb_dim", 40),
    )
    model.to(device)

    # Load pretrained checkpoint
    checkpoint = load_pretrained_checkpoint(
        model=model,
        checkpoint_path=cfg.pretrained_checkpoint,
        device=device,
        strict=False,  # Allow partial loading with warnings
    )
    
    
    # Setup optimizer
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=cfg.training.lr,
    )
    
    # Initialize reward function
    print("Initializing reward function...")
    reward_config = cfg.get("reward", {}).get("config", "large")
    reward_fn = get_recommended_reward_config(reward_config, device=device)
    
    # Initialize stat tracker for advantages
    stat_tracker = PerLabelStatTracker(global_std=cfg.training.get("global_std", False))
    
    # Create data loader
    train_loader = DataLoader(
        train_dataset,
        batch_size=cfg.training.batch_size,
        shuffle=True,
        num_workers=cfg.training.get("num_workers", 4),
    )
    
    # Training loop
    print("Starting FloWGRPO training...")
    train_iter = iter(train_loader)
    step = 0
    epoch = 0
    
    pbar = tqdm(total=cfg.training.max_steps, desc="Training")
    
    while step < cfg.training.max_steps:
        # ========== SAMPLING PHASE ==========
        model.eval()
        samples = []
        
        # Sample multiple times per batch
        num_samples_per_label = cfg.training.num_samples_per_label
        batch_size = cfg.training.batch_size
        
        try:
            batch = next(train_iter)
        except StopIteration:
            train_iter = iter(train_loader)
            batch = next(train_iter)
            epoch += 1
        
        labels = batch["label"].to(device)  # [batch_size]
        
        # Expand labels for multiple samples per label
        expanded_labels = labels.repeat_interleave(num_samples_per_label)  # [batch_size * num_samples_per_label]
        
        # Sample trajectories
        with torch.no_grad():
            final_images, log_probs_old, timesteps = sample_with_logprob(
                model,
                expanded_labels,
                num_steps=cfg.training.num_steps,
                device=device,
                return_trajectory=False,
                sigma_max=cfg.model.get("sigma_max", 80.0),
            )
        
        # Compute rewards
        # Images from sample_with_logprob are already in [0, 1] range
        images_for_reward = final_images
        
        # Create prompt strings for reward function
        reward_prompts = [f"digit {label.item()}" for label in expanded_labels]
        
        rewards = reward_fn(images_for_reward, reward_prompts)  # [batch_size * num_samples_per_label]
        
        # Visualize sampled images in wandb periodically
        if cfg.wandb.enabled and step % cfg.wandb.log_freq == 0:
            num_images_to_show = min(16, images_for_reward.shape[0])
            if num_images_to_show > 0:
                grid_pil, caption = create_image_grid(
                    images=[images_for_reward[i] for i in range(num_images_to_show)],
                    labels=[expanded_labels[i] for i in range(num_images_to_show)],
                    rewards=[rewards[i] for i in range(num_images_to_show)],
                )
                if grid_pil is not None:
                    wandb.log({
                        "sampling/generated_images": wandb.Image(grid_pil, caption=caption),
                        "sampling/image_min": images_for_reward.min().item(),
                        "sampling/image_max": images_for_reward.max().item(),
                        "sampling/image_mean": images_for_reward.mean().item(),
                        "sampling/image_std": images_for_reward.std().item(),
                    }, step=step)
        
        rewards = rewards.cpu().numpy()
        
        # ====================================================================
        # ADVANTAGE COMPUTATION (Group-Relative)
        # ====================================================================
        # FlowGRPO computes advantages relative to each label group, not globally.
        # This reduces variance when different labels have different reward scales.
        #
        # For each label group:
        #   A = (r - μ_group) / σ_group
        #
        # Where:
        #   - r: reward for a sample
        #   - μ_group: mean reward for all samples with the same label
        #   - σ_group: std dev of rewards for all samples with the same label
        #
        # The stat_tracker maintains running statistics per label across training,
        # allowing advantages to be computed relative to historical performance.
        # ====================================================================
        expanded_labels_cpu = expanded_labels.cpu().numpy()
        advantages = stat_tracker.update(expanded_labels_cpu, rewards, type='grpo')
        advantages = torch.tensor(advantages, device=device, dtype=torch.float32)
        
        # Reshape for training: [batch_size * num_samples_per_label, num_steps] -> [batch_size, num_samples_per_label, num_steps]
        log_probs_old = log_probs_old.view(batch_size, num_samples_per_label, cfg.training.num_steps)
        advantages = advantages.view(batch_size, num_samples_per_label)
        rewards = rewards.reshape(batch_size, num_samples_per_label)
        
        # Store samples for training (need to store trajectories for proper training)
        # Re-sample with trajectories stored (no grad for sampling phase)
        with torch.no_grad():
            _, trajectories_full, log_probs_old_full, timesteps_full = sample_with_logprob(
                model,
                expanded_labels,
                num_steps=cfg.training.num_steps,
                device=device,
                return_trajectory=True,
                enable_grad=False,
                sigma_max=cfg.model.get("sigma_max", 80.0),
            )
        
        # Store samples with trajectories
        for i in range(batch_size):
            for j in range(num_samples_per_label):
                sample_idx = i * num_samples_per_label + j
                samples.append({
                    "label": expanded_labels[sample_idx],
                    "log_probs_old": log_probs_old_full[sample_idx],  # [num_steps]
                    "trajectory": [traj[sample_idx] for traj in trajectories_full],  # List of [signal_dim]
                    "timesteps": timesteps_full[sample_idx],  # [num_steps]
                    "advantages": advantages[i, j],  # scalar
                    "reward": rewards[i, j],  # scalar
                    "image": final_images[sample_idx],  # [1, 28, 28]
                })
        
        # ========== TRAINING PHASE ==========
        model.train()
        
        # Shuffle samples
        np.random.shuffle(samples)
        
        # Train on samples
        num_inner_epochs = cfg.training.get("num_inner_epochs", 1)
        for inner_epoch in range(num_inner_epochs):
            # Re-shuffle for each inner epoch
            if inner_epoch > 0:
                np.random.shuffle(samples)
            
            # Process samples in mini-batches
            train_batch_size = cfg.training.get("train_batch_size", batch_size)
            for batch_start in range(0, len(samples), train_batch_size):
                batch_samples = samples[batch_start:batch_start + train_batch_size]
                
                if len(batch_samples) == 0:
                    continue
                
                # Extract batch data
                batch_labels = torch.stack([s["label"] for s in batch_samples])
                batch_log_probs_old = torch.stack([s["log_probs_old"] for s in batch_samples])  # [B, num_steps]
                batch_advantages = torch.stack([s["advantages"] for s in batch_samples])  # [B]
                
                # For each timestep, compute new log prob and GRPO loss
                num_train_timesteps = cfg.training.num_steps
                dt = 1.0 / num_train_timesteps
                
                total_loss = 0.0
                info = defaultdict(list)
                
                # Extract trajectories from samples
                trajectories_list = []
                timesteps_list = []
                for s in batch_samples:
                    trajectories_list.append(s["trajectory"])
                    timesteps_list.append(s["timesteps"])
                
                # Convert to tensors: [B, num_steps+1, MNIST_SIGNAL_DIM]
                max_len = max(len(traj) for traj in trajectories_list)
                trajectories_tensor = torch.zeros(len(batch_samples), max_len, MNIST_SIGNAL_DIM, device=device)
                for i, traj in enumerate(trajectories_list):
                    for j, state in enumerate(traj):
                        trajectories_tensor[i, j] = state
                
                timesteps_tensor = torch.stack(timesteps_list, dim=0)  # [B, num_steps]
                
                # Train on each timestep
                for j in range(num_train_timesteps):
                    # Get states at timestep j
                    x_t = trajectories_tensor[:, j]  # [B, signal_dim]
                    x_next = trajectories_tensor[:, j + 1]  # [B, signal_dim]
                    t = timesteps_tensor[:, j:j+1]  # [B, 1]
                    
                    # Compute new log probability
                    # Note: trajectories are stored backwards (t: 1.0 → 0.0), matching train0.py
                    log_prob_new = compute_log_prob_at_timestep(
                        model, x_t, t, batch_labels, x_next, dt,
                        sigma_max=cfg.model.get("sigma_max", 80.0)
                    )  # [B]
                    
                    # Get old log probability for this timestep
                    log_prob_old = batch_log_probs_old[:, j]  # [B]
                    
                    # Clip advantages
                    adv_clip_max = cfg.training.get("adv_clip_max", 10.0)
                    advantages_clipped = torch.clamp(
                        batch_advantages,
                        -adv_clip_max,
                        adv_clip_max,
                    )
                    
                    # Compute importance ratio
                    ratio = torch.exp(log_prob_new - log_prob_old)  # [B]
                    
                    # ====================================================================
                    # GRPO LOSS COMPUTATION (Clipped PPO-style)
                    # ====================================================================
                    # The GRPO loss is similar to PPO but uses group-relative advantages.
                    # 
                    # Key components:
                    # 1. Importance ratio: ratio = π_new(x_t → x_{t+1}) / π_old(x_t → x_{t+1})
                    #    where π is the policy (flow matching model) and ratio = exp(log_prob_new - log_prob_old)
                    #
                    # 2. Advantages: A = (r - μ_group) / σ_group (computed per label group)
                    #
                    # 3. Clipped objective (PPO-style):
                    #    L = E[max(
                    #        A * ratio,                    # Unclipped: standard policy gradient
                    #        A * clip(ratio, 1-ε, 1+ε)     # Clipped: prevents large updates
                    #    )]
                    #
                    # The clipping prevents the policy from changing too quickly, improving
                    # training stability. The max() ensures we take the worst-case (most
                    # conservative) update.
                    #
                    # Note: We use negative advantages because we maximize the objective
                    # (so we minimize the negative).
                    # ====================================================================
                    clip_range = cfg.training.clip_range
                    unclipped_loss = -advantages_clipped * ratio
                    clipped_loss = -advantages_clipped * torch.clamp(
                        ratio,
                        1.0 - clip_range,
                        1.0 + clip_range,
                    )
                    policy_loss = torch.mean(torch.maximum(unclipped_loss, clipped_loss))
                    
                    # Optional KL penalty (if beta > 0)
                    beta = cfg.training.get("beta", 0.0)
                    if beta > 0:
                        # Approximate KL divergence: 0.5 * (log_prob_new - log_prob_old)^2
                        kl_loss = 0.5 * torch.mean((log_prob_new - log_prob_old) ** 2)
                        loss = policy_loss + beta * kl_loss
                    else:
                        loss = policy_loss
                    
                    total_loss += loss
                    
                    # Track metrics
                    info["approx_kl"].append(0.5 * torch.mean((log_prob_new - log_prob_old) ** 2).item())
                    info["clipfrac"].append(torch.mean((torch.abs(ratio - 1.0) > clip_range).float()).item())
                    info["policy_loss"].append(policy_loss.item())
                    if beta > 0:
                        info["kl_loss"].append(kl_loss.item())
                
                # Average loss across timesteps
                total_loss = total_loss / num_train_timesteps
                
                # Backward pass
                optimizer.zero_grad()
                total_loss.backward()
                
                # Gradient clipping
                if cfg.training.max_grad_norm > 0:
                    torch.nn.utils.clip_grad_norm_(
                        model.parameters(),
                        cfg.training.max_grad_norm,
                    )
                
                optimizer.step()
                
                step += 1
                pbar.update(1)
                
                # Logging
                if cfg.wandb.enabled and step % cfg.wandb.log_freq == 0:
                    batch_rewards = [s["reward"] for s in batch_samples]
                    mean_reward = float(np.mean(batch_rewards))
                    
                    log_dict = {
                        "train/loss": float(total_loss.item()),
                        "train/policy_loss": float(np.mean(info["policy_loss"])),
                        "train/approx_kl": float(np.mean(info["approx_kl"])),
                        "train/clipfrac": float(np.mean(info["clipfrac"])),
                        "train/learning_rate": float(optimizer.param_groups[0]['lr']),
                        "train/mean_reward": mean_reward,
                        "train/mean_advantage": float(advantages_clipped.mean().item()),
                        "step": step,
                    }
                    if beta > 0:
                        log_dict["train/kl_loss"] = float(np.mean(info["kl_loss"]))

                    # Visualize sampled images during training
                    num_images_to_log = min(16, len(batch_samples))
                    grid_pil, caption = create_image_grid(
                        images=[s["image"] for s in batch_samples[:num_images_to_log]],
                        labels=[s["label"] for s in batch_samples[:num_images_to_log]],
                        rewards=[s["reward"] for s in batch_samples[:num_images_to_log]],
                        advantages=[s["advantages"] for s in batch_samples[:num_images_to_log]],
                    )
                    if grid_pil is not None:
                        log_dict["train/sampled_images"] = wandb.Image(grid_pil, caption=caption)

                        # Record image statistics from batch samples
                        batch_images = torch.stack([s["image"] for s in batch_samples])
                        log_dict["train/image_min"] = batch_images.min().item()
                        log_dict["train/image_max"] = batch_images.max().item()
                        log_dict["train/image_mean"] = batch_images.mean().item()
                        log_dict["train/image_std"] = batch_images.std().item()

                    wandb.log(log_dict, step=step)
                
                # Evaluation and checkpointing
                if step % cfg.training.eval_freq == 0:
                    # Sample some images for visualization
                    if cfg.wandb.enabled:
                        model.eval()
                        with torch.no_grad():
                            test_labels = torch.randint(0, cfg.model.vocab_size, (8,), device=device)
                            test_images, _, _ = sample_with_logprob(
                                model,
                                test_labels,
                                num_steps=cfg.training.eval_num_steps,
                                device=device,
                                sigma_max=cfg.model.get("sigma_max", 80.0),
                            )
                            # Create grid using make_grid directly (simpler for eval without rewards)
                            eval_grid = make_grid(test_images, nrow=4, normalize=False, pad_value=1.0)
                            eval_grid_np = eval_grid.permute(1, 2, 0).cpu().numpy()
                            eval_grid_np = np.clip(eval_grid_np, 0.0, 1.0)
                            eval_grid_np = (eval_grid_np * 255).astype(np.uint8)
                            eval_grid_pil = Image.fromarray(eval_grid_np, mode='RGB')

                            label_captions = ", ".join([f"L{i}:{test_labels[i].item()}" for i in range(len(test_labels))])
                            wandb.log({
                                "eval/sampled_images": wandb.Image(eval_grid_pil, caption=f"Eval - {label_captions}"),
                                "eval/image_min": test_images.min().item(),
                                "eval/image_max": test_images.max().item(),
                                "eval/image_mean": test_images.mean().item(),
                                "eval/image_std": test_images.std().item(),
                            }, step=step)
                        model.train()
                    
                    # Save checkpoint
                    checkpoint = {
                        'step': step,
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                    }
                    torch.save(checkpoint, output_dir / f'checkpoint_step_{step}.pt')
                    print(f"\nSaved checkpoint at step {step}")
                
                if step >= cfg.training.max_steps:
                    break
            
            if step >= cfg.training.max_steps:
                break
        
        if step >= cfg.training.max_steps:
            break
    
    pbar.close()
    
    # Save final model
    final_checkpoint = {
        'step': step,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
    }
    torch.save(final_checkpoint, output_dir / 'final_model.pt')
    print(f"\nTraining complete! Final model saved to {output_dir / 'final_model.pt'}")
    
    # Finish wandb run
    if cfg.wandb.enabled:
        wandb.finish()


if __name__ == "__main__":
    main()
